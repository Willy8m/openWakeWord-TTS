wakeword: hola-juan
output_dir: C:\Users\gbartag\source\Wake-Word-TTs\outputs\hola-juan/models/hola-juan
data_folder: hola-juan_v1_1_X
model_name: hola-juan_v1_1_0
augmentation_batch_size: 128
augmentation_rounds: 500
rir_paths:
- F:/data/train/mit_rirs
background_paths:
- F:/data/train/NEGATIVE/wham_noise/tr
- F:/data/train/NEGATIVE/DEMAND
background_paths_duplication_rate:
- 1
- 1
model_type: dnn
hidden_layers: 5
layer_size: 64
steps: 15000
max_negative_weight: 100
batch_n_per_class:
  positive: 64
  adversarial_negative: 64
  ACAV100M_sample: 512
  fma: 512
  podcasts: 512
  tv3_train: 512
  wham_tr: 64
feature_data_files:
  ACAV100M_sample: F:/data/train/NEGATIVE/openwakeword_features_ACAV100M_2000_hrs_16bit.npy
  fma: F:/data/train/NEGATIVE/fma_large.npy
  podcasts: F:/data/train/NEGATIVE/openwakeword_features_podcasts_10000h_ca-es_ca_es-es.npy
  tv3_train: F:/data/train/NEGATIVE/tv3_train.npy
  wham_tr: F:/data/train/NEGATIVE/wham_tr.npy
false_positive_validation_data_path: F:/datavalidation/validation_set_features.npy
target_accuracy: 0.85
target_recall: 0.25
target_false_positives_per_hour: 0.2
